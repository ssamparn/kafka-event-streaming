package com.kafka.streams.documentation;

import lombok.extern.slf4j.Slf4j;

/* *
 * Introduction to Kafka Streams API:
 * Kafka Streams is a Java library, which primarily focuses on data enrichment, which involves transformation of data, aggregating the data and joining the data from multiple topics.
 * Kafka streams API uses the functional programming style. What this means is that we will be using lambdas and operators such as map(), filter() and flatMap() that got introduced in Java 8 to build the functionality in stream applications.
 *
 * Now let's talk about how the data flows in a Kafka streams application that's built using Kafka Streams API.
 *   1. The Kafka Streams application initiates the stream processing by subscribing to one or multiple topics, which is the first step in the whole flow.
 *   2. The second step is to act on the data from the Kafka topic. This involves a different type of operations such as aggregating the data, transforming the data or joining the data etc.
 *   3. Once the processing completes, the idea is to write the data back into the Kafka topic (same or different depends on the use case) for the downstream applications that are interested in this data.
 *
 * Behind the scenes, the Stream API uses the producer and consumer API to read the data and write the data back into the Kafka topic.
 *
 * Now you might have this question. How is this different from the consumer API? Consumer API pretty much starts the whole flow by reading the data from the Kafka topic.
 * The applications that we build using consumer APIs are stateless. Now the next question is what does stateless even mean in the context of consumer API?
 * Let me explain this using the data flow in a simple consumer app. In the consumer app, what do we do?
 * We basically read an event and process the data and then move on to the next event. So consumer apps are great for notifications or processing each event independent of each other.
 * The consumer app do not have an easy way to remember what's been read, so there is no easy way to join or aggregate events. This is where streams API shine, and it has a lot of capability to build both the stateful and stateless application.
 *
 * The stateless application using Streams API is very similar to the applications that's built using Kafka Consumer API.
 * But the only difference is that Streams API uses functional programming style, which is different from the consumer API.
 *
 * Now let me talk about the stateful applications that can be built using Kafka Streams API. So if you think about retail, we can use a Kafka Streams API to build applications to calculate the total number of orders in real time.
 * Or we can build a Kafka streams applications to calculate the total revenue made in real time. The word real time is really key here because we will be able to calculate as the orders flow through the system in real time.
 * This is really powerful in today's market requirements. Let me give you another example. In the entertainment field, we can build applications to calculate the total number of tickets sold for a given movie in real time.
 * Or we can also do the total revenue generated by the movie in real time. These are fascinating use cases for stakeholders who want to know how the market is actually performing real time.
 *
 * What are the options that we have when it comes to implementation?
 * We have 2 different options available in Kafka Streams API to build streaming applications.
 *   1. Streams DSL approach: This is a high level API and it has some predefined operators available to build your streaming logic. The operators are map() flatmap() and there are many of them.
 *   2. Processor API: This is a low level API. It may not be intuitive to work with when it is compared to Streams DSL. This is a little complex compared to the streams DSL.
 *
 * Note: Streams DSL is built on top of the processor API.
 * */
@Slf4j
public class KafkaStreamsApiIntroduction {
}
